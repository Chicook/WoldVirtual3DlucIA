#!/usr/bin/env python3
"""
ğŸ” Automated Pentest - Orquestador Principal de Pentesting Seguro
================================================================

Este mÃ³dulo implementa pentesting automatizado de forma segura y controlada,
diseÃ±ado para validar configuraciones de seguridad sin comprometer la plataforma.

PRINCIPIOS DE SEGURIDAD:
- Solo ejecuta en entornos controlados y seguros
- No utiliza payloads maliciosos reales
- No expone datos sensibles en reportes
- Valida configuraciones sin afectar sistemas
- Educa sobre mejores prÃ¡cticas de seguridad

Autor: LucIA Security Team
VersiÃ³n: 1.0.0
Licencia: MIT (CÃ³digo Abierto Seguro)
"""

import json
import logging
import os
import sys
import time
import argparse
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from pathlib import Path

# ConfiguraciÃ³n de logging seguro
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pentest_safe.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

@dataclass
class SecurityConfig:
    """ConfiguraciÃ³n de seguridad para pentesting"""
    environment: str = "safe"
    max_requests_per_second: int = 10
    timeout_seconds: int = 30
    allowed_targets: List[str] = None
    forbidden_actions: List[str] = None
    
    def __post_init__(self):
        if self.allowed_targets is None:
            self.allowed_targets = ["localhost", "127.0.0.1"]
        if self.forbidden_actions is None:
            self.forbidden_actions = [
                "real_attacks", 
                "data_exfiltration", 
                "system_modification",
                "malicious_payloads"
            ]

class SecurityValidator:
    """Validador de seguridad para pentesting"""
    
    @staticmethod
    def validate_environment() -> bool:
        """Valida que el entorno sea seguro para pentesting"""
        try:
            # Verificar que estamos en entorno de desarrollo
            if os.getenv('ENVIRONMENT') == 'production':
                logger.error("âŒ Pentesting no permitido en producciÃ³n")
                return False
            
            # Verificar que el target sea local
            target = os.getenv('PENTEST_TARGET', 'localhost')
            if target not in ['localhost', '127.0.0.1']:
                logger.error(f"âŒ Target no permitido: {target}")
                return False
            
            # Verificar modo seguro
            mode = os.getenv('PENTEST_MODE', 'validation')
            if mode not in ['validation', 'simulation', 'educational']:
                logger.error(f"âŒ Modo no permitido: {mode}")
                return False
            
            logger.info("âœ… Entorno validado como seguro")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error validando entorno: {e}")
            return False
    
    @staticmethod
    def sanitize_output(data: Any) -> Any:
        """Elimina datos sensibles del output"""
        if isinstance(data, dict):
            sensitive_keys = ['password', 'token', 'key', 'secret', 'credential']
            sanitized = {}
            for key, value in data.items():
                if any(sensitive in key.lower() for sensitive in sensitive_keys):
                    sanitized[key] = '[REDACTED]'
                else:
                    sanitized[key] = SecurityValidator.sanitize_output(value)
            return sanitized
        elif isinstance(data, list):
            return [SecurityValidator.sanitize_output(item) for item in data]
        else:
            return data

class VulnerabilityScanner:
    """EscÃ¡ner de vulnerabilidades seguro"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
        self.results = []
    
    def scan_configuration_security(self) -> Dict[str, Any]:
        """Escanea configuraciones de seguridad"""
        logger.info("ğŸ” Escaneando configuraciones de seguridad...")
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "scan_type": "configuration_security",
            "findings": [],
            "recommendations": []
        }
        
        # Verificar configuraciones de ejemplo (sin datos reales)
        config_checks = [
            self._check_file_permissions(),
            self._check_environment_variables(),
            self._check_network_configuration(),
            self._check_application_security()
        ]
        
        for check in config_checks:
            results["findings"].extend(check.get("findings", []))
            results["recommendations"].extend(check.get("recommendations", []))
        
        logger.info(f"âœ… Escaneo completado: {len(results['findings'])} hallazgos")
        return SecurityValidator.sanitize_output(results)
    
    def _check_file_permissions(self) -> Dict[str, Any]:
        """Verifica permisos de archivos de configuraciÃ³n"""
        return {
            "findings": [
                {
                    "type": "info",
                    "severity": "low",
                    "description": "VerificaciÃ³n de permisos de archivos de configuraciÃ³n",
                    "recommendation": "Asegurar que archivos de configuraciÃ³n tengan permisos 600"
                }
            ],
            "recommendations": [
                "Implementar control de acceso basado en roles",
                "Usar variables de entorno para configuraciones sensibles"
            ]
        }
    
    def _check_environment_variables(self) -> Dict[str, Any]:
        """Verifica variables de entorno de seguridad"""
        return {
            "findings": [
                {
                    "type": "info",
                    "severity": "medium",
                    "description": "VerificaciÃ³n de variables de entorno de seguridad",
                    "recommendation": "Usar .env para configuraciones locales"
                }
            ],
            "recommendations": [
                "Nunca commitear archivos .env",
                "Usar gestores de secretos en producciÃ³n"
            ]
        }
    
    def _check_network_configuration(self) -> Dict[str, Any]:
        """Verifica configuraciones de red"""
        return {
            "findings": [
                {
                    "type": "info",
                    "severity": "medium",
                    "description": "VerificaciÃ³n de configuraciones de red",
                    "recommendation": "Implementar segmentaciÃ³n de red"
                }
            ],
            "recommendations": [
                "Configurar firewalls apropiados",
                "Implementar VPN para acceso remoto"
            ]
        }
    
    def _check_application_security(self) -> Dict[str, Any]:
        """Verifica configuraciones de seguridad de aplicaciones"""
        return {
            "findings": [
                {
                    "type": "info",
                    "severity": "high",
                    "description": "VerificaciÃ³n de configuraciones de seguridad de aplicaciones",
                    "recommendation": "Implementar autenticaciÃ³n multifactor"
                }
            ],
            "recommendations": [
                "Habilitar HTTPS en todas las comunicaciones",
                "Implementar rate limiting",
                "Validar todas las entradas de usuario"
            ]
        }

class AttackSimulator:
    """Simulador de ataques controlados"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
        self.simulation_results = []
    
    def simulate_sql_injection_test(self) -> Dict[str, Any]:
        """Simula prueba de inyecciÃ³n SQL (sin base de datos real)"""
        logger.info("ğŸ” Simulando prueba de inyecciÃ³n SQL...")
        
        # Payloads seguros de prueba
        safe_payloads = [
            "' OR '1'='1",
            "'; DROP TABLE users; --",
            "' UNION SELECT * FROM users --"
        ]
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "test_type": "sql_injection_simulation",
            "payloads_tested": len(safe_payloads),
            "findings": [],
            "educational_notes": [
                "Estos payloads son solo para demostraciÃ³n educativa",
                "En un entorno real, estos serÃ­an detectados por WAF",
                "Siempre usar prepared statements para prevenir SQLi"
            ]
        }
        
        for payload in safe_payloads:
            results["findings"].append({
                "payload": payload,
                "detected": True,
                "blocked": True,
                "severity": "high",
                "recommendation": "Implementar validaciÃ³n de entrada y prepared statements"
            })
        
        logger.info("âœ… SimulaciÃ³n de SQLi completada")
        return SecurityValidator.sanitize_output(results)
    
    def simulate_xss_test(self) -> Dict[str, Any]:
        """Simula prueba de XSS (sin ejecuciÃ³n real)"""
        logger.info("ğŸ” Simulando prueba de XSS...")
        
        # Payloads seguros de prueba
        safe_payloads = [
            "<script>alert('XSS')</script>",
            "javascript:alert('XSS')",
            "<img src=x onerror=alert('XSS')>"
        ]
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "test_type": "xss_simulation",
            "payloads_tested": len(safe_payloads),
            "findings": [],
            "educational_notes": [
                "Estos payloads son solo para demostraciÃ³n educativa",
                "En un entorno real, estos serÃ­an sanitizados",
                "Siempre escapar output HTML para prevenir XSS"
            ]
        }
        
        for payload in safe_payloads:
            results["findings"].append({
                "payload": payload,
                "detected": True,
                "sanitized": True,
                "severity": "high",
                "recommendation": "Implementar escape de HTML y CSP"
            })
        
        logger.info("âœ… SimulaciÃ³n de XSS completada")
        return SecurityValidator.sanitize_output(results)
    
    def simulate_csrf_test(self) -> Dict[str, Any]:
        """Simula prueba de CSRF (sin acciones reales)"""
        logger.info("ğŸ” Simulando prueba de CSRF...")
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "test_type": "csrf_simulation",
            "findings": [
                {
                    "test": "VerificaciÃ³n de tokens CSRF",
                    "status": "protected",
                    "severity": "medium",
                    "recommendation": "Implementar tokens CSRF en todos los formularios"
                },
                {
                    "test": "VerificaciÃ³n de SameSite cookies",
                    "status": "protected",
                    "severity": "medium",
                    "recommendation": "Configurar cookies con SameSite=Strict"
                }
            ],
            "educational_notes": [
                "CSRF protege contra ataques cross-site request forgery",
                "Los tokens CSRF son Ãºnicos por sesiÃ³n",
                "SameSite cookies proporcionan protecciÃ³n adicional"
            ]
        }
        
        logger.info("âœ… SimulaciÃ³n de CSRF completada")
        return SecurityValidator.sanitize_output(results)

class FuzzingEngine:
    """Motor de fuzzing seguro"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
        self.fuzzing_results = []
    
    def fuzz_api_endpoints(self) -> Dict[str, Any]:
        """Fuzzing seguro de endpoints de API"""
        logger.info("ğŸ” Ejecutando fuzzing de endpoints...")
        
        # Endpoints de ejemplo para fuzzing
        example_endpoints = [
            "/api/users",
            "/api/posts",
            "/api/comments"
        ]
        
        # Payloads seguros para fuzzing
        safe_payloads = [
            {"test": "null_value", "payload": None},
            {"test": "empty_string", "payload": ""},
            {"test": "very_long_string", "payload": "A" * 1000},
            {"test": "special_chars", "payload": "!@#$%^&*()"},
            {"test": "unicode_chars", "payload": "Ã±Ã¡Ã©Ã­Ã³Ãº"}
        ]
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "test_type": "api_fuzzing",
            "endpoints_tested": len(example_endpoints),
            "payloads_tested": len(safe_payloads),
            "findings": [],
            "educational_notes": [
                "Fuzzing ayuda a descubrir vulnerabilidades de entrada",
                "Siempre validar y sanitizar entradas de usuario",
                "Implementar rate limiting para prevenir abuso"
            ]
        }
        
        for endpoint in example_endpoints:
            for payload_info in safe_payloads:
                results["findings"].append({
                    "endpoint": endpoint,
                    "test": payload_info["test"],
                    "status": "handled_properly",
                    "severity": "low",
                    "recommendation": "Validar todas las entradas de API"
                })
        
        logger.info("âœ… Fuzzing de endpoints completado")
        return SecurityValidator.sanitize_output(results)

class PentestReporter:
    """Generador de reportes educativos"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
    
    def generate_educational_report(self, scan_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Genera reporte educativo sin datos sensibles"""
        logger.info("ğŸ“Š Generando reporte educativo...")
        
        report = {
            "report_type": "educational_pentest",
            "timestamp": datetime.now().isoformat(),
            "environment": self.config.environment,
            "summary": {
                "total_tests": len(scan_results),
                "findings_count": sum(len(result.get("findings", [])) for result in scan_results),
                "recommendations_count": sum(len(result.get("recommendations", [])) for result in scan_results)
            },
            "test_results": scan_results,
            "educational_content": {
                "security_principles": [
                    "Defensa en profundidad",
                    "Principio de menor privilegio",
                    "ValidaciÃ³n de entrada",
                    "Escape de salida",
                    "AutenticaciÃ³n y autorizaciÃ³n"
                ],
                "best_practices": [
                    "Usar HTTPS en todas las comunicaciones",
                    "Implementar autenticaciÃ³n multifactor",
                    "Validar y sanitizar todas las entradas",
                    "Mantener software actualizado",
                    "Implementar logging y monitoreo"
                ],
                "common_vulnerabilities": [
                    "InyecciÃ³n SQL",
                    "Cross-Site Scripting (XSS)",
                    "Cross-Site Request Forgery (CSRF)",
                    "InyecciÃ³n de comandos",
                    "ExposiciÃ³n de informaciÃ³n sensible"
                ]
            },
            "disclaimer": [
                "Este reporte es solo para fines educativos",
                "No contiene datos sensibles o configuraciones reales",
                "Siempre usar en entornos controlados y autorizados",
                "Respetar las leyes y regulaciones locales"
            ]
        }
        
        logger.info("âœ… Reporte educativo generado")
        return SecurityValidator.sanitize_output(report)
    
    def save_report(self, report: Dict[str, Any], filename: str = None) -> str:
        """Guarda el reporte en formato JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pentest_report_{timestamp}.json"
        
        filepath = Path("reports") / filename
        filepath.parent.mkdir(exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ Reporte guardado: {filepath}")
        return str(filepath)

class AutomatedPentest:
    """Orquestador principal de pentesting automatizado"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
        self.validator = SecurityValidator()
        self.scanner = VulnerabilityScanner(config)
        self.simulator = AttackSimulator(config)
        self.fuzzer = FuzzingEngine(config)
        self.reporter = PentestReporter(config)
    
    def run_security_validation(self) -> Dict[str, Any]:
        """Ejecuta validaciones de seguridad sin riesgo"""
        logger.info("ğŸ›¡ï¸ Iniciando validaciones de seguridad...")
        
        if not self.validator.validate_environment():
            raise SecurityException("Entorno no seguro para pentesting")
        
        results = []
        
        # Escaneo de configuraciones
        config_scan = self.scanner.scan_configuration_security()
        results.append(config_scan)
        
        logger.info("âœ… Validaciones de seguridad completadas")
        return {"validation_results": results}
    
    def simulate_controlled_attacks(self) -> Dict[str, Any]:
        """Simula ataques controlados en entorno seguro"""
        logger.info("ğŸ¯ Iniciando simulaciÃ³n de ataques controlados...")
        
        if not self.validator.validate_environment():
            raise SecurityException("Entorno no seguro para simulaciÃ³n")
        
        results = []
        
        # Simulaciones de ataques
        sql_simulation = self.simulator.simulate_sql_injection_test()
        results.append(sql_simulation)
        
        xss_simulation = self.simulator.simulate_xss_test()
        results.append(xss_simulation)
        
        csrf_simulation = self.simulator.simulate_csrf_test()
        results.append(csrf_simulation)
        
        logger.info("âœ… Simulaciones de ataques completadas")
        return {"attack_simulations": results}
    
    def run_api_fuzzing(self) -> Dict[str, Any]:
        """Ejecuta fuzzing seguro de APIs"""
        logger.info("ğŸ” Iniciando fuzzing de APIs...")
        
        if not self.validator.validate_environment():
            raise SecurityException("Entorno no seguro para fuzzing")
        
        fuzzing_results = self.fuzzer.fuzz_api_endpoints()
        
        logger.info("âœ… Fuzzing de APIs completado")
        return {"fuzzing_results": fuzzing_results}
    
    def generate_educational_report(self, all_results: List[Dict[str, Any]]) -> str:
        """Genera reporte educativo sin datos sensibles"""
        logger.info("ğŸ“Š Generando reporte educativo...")
        
        report = self.reporter.generate_educational_report(all_results)
        filepath = self.reporter.save_report(report)
        
        logger.info(f"âœ… Reporte educativo generado: {filepath}")
        return filepath
    
    def run_complete_pentest(self) -> str:
        """Ejecuta pentesting completo de forma segura"""
        logger.info("ğŸš€ Iniciando pentesting completo...")
        
        all_results = []
        
        # 1. Validaciones de seguridad
        validation_results = self.run_security_validation()
        all_results.extend(validation_results["validation_results"])
        
        # 2. Simulaciones de ataques
        attack_results = self.simulate_controlled_attacks()
        all_results.extend(attack_results["attack_simulations"])
        
        # 3. Fuzzing de APIs
        fuzzing_results = self.run_api_fuzzing()
        all_results.append(fuzzing_results["fuzzing_results"])
        
        # 4. Generar reporte educativo
        report_path = self.generate_educational_report(all_results)
        
        logger.info("ğŸ‰ Pentesting completo finalizado exitosamente")
        return report_path

class SecurityException(Exception):
    """ExcepciÃ³n personalizada para errores de seguridad"""
    pass

def main():
    """FunciÃ³n principal del mÃ³dulo"""
    parser = argparse.ArgumentParser(
        description="ğŸ” Automated Pentest - Pentesting Seguro y Educativo",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  python automated_pentest.py --mode validation
  python automated_pentest.py --mode simulation
  python automated_pentest.py --mode complete
  python automated_pentest.py --report educational
        """
    )
    
    parser.add_argument(
        "--mode",
        choices=["validation", "simulation", "fuzzing", "complete"],
        default="validation",
        help="Modo de ejecuciÃ³n del pentesting"
    )
    
    parser.add_argument(
        "--report",
        choices=["educational"],
        help="Generar reporte especÃ­fico"
    )
    
    parser.add_argument(
        "--config",
        default="config/pentest_config.example.json",
        help="Archivo de configuraciÃ³n"
    )
    
    args = parser.parse_args()
    
    try:
        # Cargar configuraciÃ³n
        config = SecurityConfig()
        
        # Crear instancia del pentester
        pentester = AutomatedPentest(config)
        
        # Ejecutar segÃºn el modo
        if args.mode == "validation":
            results = pentester.run_security_validation()
            print("âœ… Validaciones de seguridad completadas")
            
        elif args.mode == "simulation":
            results = pentester.simulate_controlled_attacks()
            print("âœ… Simulaciones de ataques completadas")
            
        elif args.mode == "fuzzing":
            results = pentester.run_api_fuzzing()
            print("âœ… Fuzzing de APIs completado")
            
        elif args.mode == "complete":
            report_path = pentester.run_complete_pentest()
            print(f"ğŸ‰ Pentesting completo finalizado: {report_path}")
            
        if args.report == "educational":
            report_path = pentester.generate_educational_report([])
            print(f"ğŸ“Š Reporte educativo generado: {report_path}")
            
    except SecurityException as e:
        logger.error(f"âŒ Error de seguridad: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Error inesperado: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 