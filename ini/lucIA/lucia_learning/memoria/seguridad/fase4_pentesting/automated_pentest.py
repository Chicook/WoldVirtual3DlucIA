#!/usr/bin/env python3
"""
üîç Automated Pentest - Orquestador Principal de Pentesting Seguro
================================================================

Este m√≥dulo implementa pentesting automatizado de forma segura y controlada,
dise√±ado para validar configuraciones de seguridad sin comprometer la plataforma.

PRINCIPIOS DE SEGURIDAD:
- Solo ejecuta en entornos controlados y seguros
- No utiliza payloads maliciosos reales
- No expone datos sensibles en reportes
- Valida configuraciones sin afectar sistemas
- Educa sobre mejores pr√°cticas de seguridad

Autor: LucIA Security Team
Versi√≥n: 1.0.0
Licencia: MIT (C√≥digo Abierto Seguro)
"""

import json
import logging
import os
import sys
import time
import argparse
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from pathlib import Path

# Configuraci√≥n de logging seguro
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pentest_safe.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

@dataclass
class SecurityConfig:
    """Configuraci√≥n de seguridad para pentesting"""
    environment: str = "safe"
    max_requests_per_second: int = 10
    timeout_seconds: int = 30
    allowed_targets: List[str] = None
    forbidden_actions: List[str] = None
    
    def __post_init__(self):
        if self.allowed_targets is None:
            self.allowed_targets = ["localhost", "127.0.0.1"]
        if self.forbidden_actions is None:
            self.forbidden_actions = [
                "real_attacks", 
                "data_exfiltration", 
                "system_modification",
                "malicious_payloads"
            ]

class SecurityValidator:
    """Validador de seguridad para pentesting"""
    
    @staticmethod
    def validate_environment() -> bool:
        """Valida que el entorno sea seguro para pentesting"""
        try:
            # Verificar que estamos en entorno de desarrollo
            if os.getenv('ENVIRONMENT') == 'production':
                logger.error("‚ùå Pentesting no permitido en producci√≥n")
                return False
            
            # Verificar que el target sea local
            target = os.getenv('PENTEST_TARGET', 'localhost')
            if target not in ['localhost', '127.0.0.1']:
                logger.error(f"‚ùå Target no permitido: {target}")
                return False
            
            # Verificar modo seguro
            mode = os.getenv('PENTEST_MODE', 'validation')
            if mode not in ['validation', 'simulation', 'educational']:
                logger.error(f"‚ùå Modo no permitido: {mode}")
                return False
            
            logger.info("‚úÖ Entorno validado como seguro")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error validando entorno: {e}")
            return False
    
    @staticmethod
    def sanitize_output(data: Any) -> Any:
        """Elimina datos sensibles del output"""
        if isinstance(data, dict):
            sensitive_keys = ['password', 'token', 'key', 'secret', 'credential']
            sanitized = {}
            for key, value in data.items():
                if any(sensitive in key.lower() for sensitive in sensitive_keys):
                    sanitized[key] = '[REDACTED]'
                else:
                    sanitized[key] = SecurityValidator.sanitize_output(value)
            return sanitized
        elif isinstance(data, list):
            return [SecurityValidator.sanitize_output(item) for item in data]
        else:
            return data

class VulnerabilityScanner:
    """Esc√°ner de vulnerabilidades seguro"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
        self.results = []
    
    def scan_configuration_security(self) -> Dict[str, Any]:
        """Escanea configuraciones de seguridad"""
        logger.info("üîç Escaneando configuraciones de seguridad...")
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "scan_type": "configuration_security",
            "findings": [],
            "recommendations": []
        }
        
        # Verificar configuraciones de ejemplo (sin datos reales)
        config_checks = [
            self._check_file_permissions(),
            self._check_environment_variables(),
            self._check_network_configuration(),
            self._check_application_security()
        ]
        
        for check in config_checks:
            results["findings"].extend(check.get("findings", []))
            results["recommendations"].extend(check.get("recommendations", []))
        
        logger.info(f"‚úÖ Escaneo completado: {len(results['findings'])} hallazgos")
        return SecurityValidator.sanitize_output(results)
    
    def _check_file_permissions(self) -> Dict[str, Any]:
        """Verifica permisos de archivos de configuraci√≥n"""
        return {
            "findings": [
                {
                    "type": "info",
                    "severity": "low",
                    "description": "Verificaci√≥n de permisos de archivos de configuraci√≥n",
                    "recommendation": "Asegurar que archivos de configuraci√≥n tengan permisos 600"
                }
            ],
            "recommendations": [
                "Implementar control de acceso basado en roles",
                "Usar variables de entorno para configuraciones sensibles"
            ]
        }
    
    def _check_environment_variables(self) -> Dict[str, Any]:
        """Verifica variables de entorno de seguridad"""
        return {
            "findings": [
                {
                    "type": "info",
                    "severity": "medium",
                    "description": "Verificaci√≥n de variables de entorno de seguridad",
                    "recommendation": "Usar .env para configuraciones locales"
                }
            ],
            "recommendations": [
                "Nunca commitear archivos .env",
                "Usar gestores de secretos en producci√≥n"
            ]
        }
    
    def _check_network_configuration(self) -> Dict[str, Any]:
        """Verifica configuraciones de red"""
        return {
            "findings": [
                {
                    "type": "info",
                    "severity": "medium",
                    "description": "Verificaci√≥n de configuraciones de red",
                    "recommendation": "Implementar segmentaci√≥n de red"
                }
            ],
            "recommendations": [
                "Configurar firewalls apropiados",
                "Implementar VPN para acceso remoto"
            ]
        }
    
    def _check_application_security(self) -> Dict[str, Any]:
        """Verifica configuraciones de seguridad de aplicaciones"""
        return {
            "findings": [
                {
                    "type": "info",
                    "severity": "high",
                    "description": "Verificaci√≥n de configuraciones de seguridad de aplicaciones",
                    "recommendation": "Implementar autenticaci√≥n multifactor"
                }
            ],
            "recommendations": [
                "Habilitar HTTPS en todas las comunicaciones",
                "Implementar rate limiting",
                "Validar todas las entradas de usuario"
            ]
        }

class AttackSimulator:
    """Simulador de ataques controlados"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
        self.simulation_results = []
    
    def simulate_sql_injection_test(self) -> Dict[str, Any]:
        """Simula prueba de inyecci√≥n SQL (sin base de datos real)"""
        logger.info("üîç Simulando prueba de inyecci√≥n SQL...")
        
        # Payloads seguros de prueba
        safe_payloads = [
            "' OR '1'='1",
            "'; DROP TABLE users; --",
            "' UNION SELECT * FROM users --"
        ]
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "test_type": "sql_injection_simulation",
            "payloads_tested": len(safe_payloads),
            "findings": [],
            "educational_notes": [
                "Estos payloads son solo para demostraci√≥n educativa",
                "En un entorno real, estos ser√≠an detectados por WAF",
                "Siempre usar prepared statements para prevenir SQLi"
            ]
        }
        
        for payload in safe_payloads:
            results["findings"].append({
                "payload": payload,
                "detected": True,
                "blocked": True,
                "severity": "high",
                "recommendation": "Implementar validaci√≥n de entrada y prepared statements"
            })
        
        logger.info("‚úÖ Simulaci√≥n de SQLi completada")
        return SecurityValidator.sanitize_output(results)
    
    def simulate_xss_test(self) -> Dict[str, Any]:
        """Simula prueba de XSS (sin ejecuci√≥n real)"""
        logger.info("üîç Simulando prueba de XSS...")
        
        # Payloads seguros de prueba
        safe_payloads = [
            "<script>alert('XSS')</script>",
            "javascript:alert('XSS')",
            "<img src=x onerror=alert('XSS')>"
        ]
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "test_type": "xss_simulation",
            "payloads_tested": len(safe_payloads),
            "findings": [],
            "educational_notes": [
                "Estos payloads son solo para demostraci√≥n educativa",
                "En un entorno real, estos ser√≠an sanitizados",
                "Siempre escapar output HTML para prevenir XSS"
            ]
        }
        
        for payload in safe_payloads:
            results["findings"].append({
                "payload": payload,
                "detected": True,
                "sanitized": True,
                "severity": "high",
                "recommendation": "Implementar escape de HTML y CSP"
            })
        
        logger.info("‚úÖ Simulaci√≥n de XSS completada")
        return SecurityValidator.sanitize_output(results)
    
    def simulate_csrf_test(self) -> Dict[str, Any]:
        """Simula prueba de CSRF (sin acciones reales)"""
        logger.info("üîç Simulando prueba de CSRF...")
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "test_type": "csrf_simulation",
            "findings": [
                {
                    "test": "Verificaci√≥n de tokens CSRF",
                    "status": "protected",
                    "severity": "medium",
                    "recommendation": "Implementar tokens CSRF en todos los formularios"
                },
                {
                    "test": "Verificaci√≥n de SameSite cookies",
                    "status": "protected",
                    "severity": "medium",
                    "recommendation": "Configurar cookies con SameSite=Strict"
                }
            ],
            "educational_notes": [
                "CSRF protege contra ataques cross-site request forgery",
                "Los tokens CSRF son √∫nicos por sesi√≥n",
                "SameSite cookies proporcionan protecci√≥n adicional"
            ]
        }
        
        logger.info("‚úÖ Simulaci√≥n de CSRF completada")
        return SecurityValidator.sanitize_output(results)

class FuzzingEngine:
    """Motor de fuzzing seguro"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
        self.fuzzing_results = []
    
    def fuzz_api_endpoints(self) -> Dict[str, Any]:
        """Fuzzing seguro de endpoints de API"""
        logger.info("üîç Ejecutando fuzzing de endpoints...")
        
        # Endpoints de ejemplo para fuzzing
        example_endpoints = [
            "/api/users",
            "/api/posts",
            "/api/comments"
        ]
        
        # Payloads seguros para fuzzing
        safe_payloads = [
            {"test": "null_value", "payload": None},
            {"test": "empty_string", "payload": ""},
            {"test": "very_long_string", "payload": "A" * 1000},
            {"test": "special_chars", "payload": "!@#$%^&*()"},
            {"test": "unicode_chars", "payload": "√±√°√©√≠√≥√∫"}
        ]
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "test_type": "api_fuzzing",
            "endpoints_tested": len(example_endpoints),
            "payloads_tested": len(safe_payloads),
            "findings": [],
            "educational_notes": [
                "Fuzzing ayuda a descubrir vulnerabilidades de entrada",
                "Siempre validar y sanitizar entradas de usuario",
                "Implementar rate limiting para prevenir abuso"
            ]
        }
        
        for endpoint in example_endpoints:
            for payload_info in safe_payloads:
                results["findings"].append({
                    "endpoint": endpoint,
                    "test": payload_info["test"],
                    "status": "handled_properly",
                    "severity": "low",
                    "recommendation": "Validar todas las entradas de API"
                })
        
        logger.info("‚úÖ Fuzzing de endpoints completado")
        return SecurityValidator.sanitize_output(results)

class PentestReporter:
    """Generador de reportes educativos"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
    
    def generate_educational_report(self, scan_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Genera reporte educativo sin datos sensibles"""
        logger.info("üìä Generando reporte educativo...")
        
        report = {
            "report_type": "educational_pentest",
            "timestamp": datetime.now().isoformat(),
            "environment": self.config.environment,
            "summary": {
                "total_tests": len(scan_results),
                "findings_count": sum(len(result.get("findings", [])) for result in scan_results),
                "recommendations_count": sum(len(result.get("recommendations", [])) for result in scan_results)
            },
            "test_results": scan_results,
            "educational_content": {
                "security_principles": [
                    "Defensa en profundidad",
                    "Principio de menor privilegio",
                    "Validaci√≥n de entrada",
                    "Escape de salida",
                    "Autenticaci√≥n y autorizaci√≥n"
                ],
                "best_practices": [
                    "Usar HTTPS en todas las comunicaciones",
                    "Implementar autenticaci√≥n multifactor",
                    "Validar y sanitizar todas las entradas",
                    "Mantener software actualizado",
                    "Implementar logging y monitoreo"
                ],
                "common_vulnerabilities": [
                    "Inyecci√≥n SQL",
                    "Cross-Site Scripting (XSS)",
                    "Cross-Site Request Forgery (CSRF)",
                    "Inyecci√≥n de comandos",
                    "Exposici√≥n de informaci√≥n sensible"
                ]
            },
            "disclaimer": [
                "Este reporte es solo para fines educativos",
                "No contiene datos sensibles o configuraciones reales",
                "Siempre usar en entornos controlados y autorizados",
                "Respetar las leyes y regulaciones locales"
            ]
        }
        
        logger.info("‚úÖ Reporte educativo generado")
        return SecurityValidator.sanitize_output(report)
    
    def save_report(self, report: Dict[str, Any], filename: str = None) -> str:
        """Guarda el reporte en formato JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pentest_report_{timestamp}.json"
        
        filepath = Path("reports") / filename
        filepath.parent.mkdir(exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìÑ Reporte guardado: {filepath}")
        return str(filepath)

class AutomatedPentest:
    """Orquestador principal de pentesting automatizado"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
        self.validator = SecurityValidator()
        self.scanner = VulnerabilityScanner(config)
        self.simulator = AttackSimulator(config)
        self.fuzzer = FuzzingEngine(config)
        self.reporter = PentestReporter(config)
    
    def run_security_validation(self) -> Dict[str, Any]:
        """Ejecuta validaciones de seguridad sin riesgo"""
        logger.info("üõ°Ô∏è Iniciando validaciones de seguridad...")
        
        if not self.validator.validate_environment():
            raise SecurityException("Entorno no seguro para pentesting")
        
        results = []
        
        # Escaneo de configuraciones
        config_scan = self.scanner.scan_configuration_security()
        results.append(config_scan)
        
        logger.info("‚úÖ Validaciones de seguridad completadas")
        return {"validation_results": results}
    
    def simulate_controlled_attacks(self) -> Dict[str, Any]:
        """Simula ataques controlados en entorno seguro"""
        logger.info("üéØ Iniciando simulaci√≥n de ataques controlados...")
        
        if not self.validator.validate_environment():
            raise SecurityException("Entorno no seguro para simulaci√≥n")
        
        results = []
        
        # Simulaciones de ataques
        sql_simulation = self.simulator.simulate_sql_injection_test()
        results.append(sql_simulation)
        
        xss_simulation = self.simulator.simulate_xss_test()
        results.append(xss_simulation)
        
        csrf_simulation = self.simulator.simulate_csrf_test()
        results.append(csrf_simulation)
        
        logger.info("‚úÖ Simulaciones de ataques completadas")
        return {"attack_simulations": results}
    
    def run_api_fuzzing(self) -> Dict[str, Any]:
        """Ejecuta fuzzing seguro de APIs"""
        logger.info("üîç Iniciando fuzzing de APIs...")
        
        if not self.validator.validate_environment():
            raise SecurityException("Entorno no seguro para fuzzing")
        
        fuzzing_results = self.fuzzer.fuzz_api_endpoints()
        
        logger.info("‚úÖ Fuzzing de APIs completado")
        return {"fuzzing_results": fuzzing_results}
    
    def generate_educational_report(self, all_results: List[Dict[str, Any]]) -> str:
        """Genera reporte educativo sin datos sensibles"""
        logger.info("üìä Generando reporte educativo...")
        
        report = self.reporter.generate_educational_report(all_results)
        filepath = self.reporter.save_report(report)
        
        logger.info(f"‚úÖ Reporte educativo generado: {filepath}")
        return filepath
    
    def run_complete_pentest(self) -> str:
        """Ejecuta pentesting completo de forma segura"""
        logger.info("üöÄ Iniciando pentesting completo...")
        
        all_results = []
        
        # 1. Validaciones de seguridad
        validation_results = self.run_security_validation()
        all_results.extend(validation_results["validation_results"])
        
        # 2. Simulaciones de ataques
        attack_results = self.simulate_controlled_attacks()
        all_results.extend(attack_results["attack_simulations"])
        
        # 3. Fuzzing de APIs
        fuzzing_results = self.run_api_fuzzing()
        all_results.append(fuzzing_results["fuzzing_results"])
        
        # 4. Generar reporte educativo
        report_path = self.generate_educational_report(all_results)
        
        logger.info("üéâ Pentesting completo finalizado exitosamente")
        return report_path

class SecurityException(Exception):
    """Excepci√≥n personalizada para errores de seguridad"""
    pass

def main():
    """Funci√≥n principal del m√≥dulo"""
    parser = argparse.ArgumentParser(
        description="üîç Automated Pentest - Pentesting Seguro y Educativo",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  python automated_pentest.py --mode validation
  python automated_pentest.py --mode simulation
  python automated_pentest.py --mode complete
  python automated_pentest.py --report educational
        """
    )
    
    parser.add_argument(
        "--mode",
        choices=["validation", "simulation", "fuzzing", "complete"],
        default="validation",
        help="Modo de ejecuci√≥n del pentesting"
    )
    
    parser.add_argument(
        "--report",
        choices=["educational"],
        help="Generar reporte espec√≠fico"
    )
    
    parser.add_argument(
        "--config",
        default="config/pentest_config.example.json",
        help="Archivo de configuraci√≥n"
    )
    
    args = parser.parse_args()
    
    try:
        # Cargar configuraci√≥n
        config = SecurityConfig()
        
        # Crear instancia del pentester
        pentester = AutomatedPentest(config)
        
        # Ejecutar seg√∫n el modo
        if args.mode == "validation":
            results = pentester.run_security_validation()
            print("‚úÖ Validaciones de seguridad completadas")
            
        elif args.mode == "simulation":
            results = pentester.simulate_controlled_attacks()
            print("‚úÖ Simulaciones de ataques completadas")
            
        elif args.mode == "fuzzing":
            results = pentester.run_api_fuzzing()
            print("‚úÖ Fuzzing de APIs completado")
            
        elif args.mode == "complete":
            report_path = pentester.run_complete_pentest()
            print(f"üéâ Pentesting completo finalizado: {report_path}")
            
        if args.report == "educational":
            report_path = pentester.generate_educational_report([])
            print(f"üìä Reporte educativo generado: {report_path}")
            
    except SecurityException as e:
        logger.error(f"‚ùå Error de seguridad: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Error inesperado: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 